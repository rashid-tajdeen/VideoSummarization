@Article{Gharahbagh2022,
  author         = {Gharahbagh, Abdorreza Alavi and Hajihashemi, Vahid and Ferreira, Marta Campos and Machado, José J. M. and Tavares, João Manuel R. S.},
  journal        = {Applied Sciences},
  title          = {Best Frame Selection to Enhance Training Step Efficiency in Video-Based Human Action Recognition},
  year           = {2022},
  issn           = {2076-3417},
  number         = {4},
  volume         = {12},
  abstract       = {In recent years, with the growth of digital media and modern imaging equipment, the use of video processing algorithms and semantic film and image management has expanded. The usage of different video datasets in training artificial intelligence algorithms is also rapidly expanding in various fields. Due to the high volume of information in a video, its processing is still expensive for most hardware systems, mainly in terms of its required runtime and memory. Hence, the optimal selection of keyframes to minimize redundant information in video processing systems has become noteworthy in facilitating this problem. Eliminating some frames can simultaneously reduce the required computational load, hardware cost, memory and processing time of intelligent video-based systems. Based on the aforementioned reasons, this research proposes a method for selecting keyframes and adaptive cropping input video for human action recognition (HAR) systems. The proposed method combines edge detection, simple difference, adaptive thresholding and 1D and 2D average filter algorithms in a hierarchical method. Some HAR methods are trained with videos processed by the proposed method to assess its efficiency. The results demonstrate that the application of the proposed method increases the accuracy of the HAR system by up to 3% compared to random image selection and cropping methods. Additionally, for most cases, the proposed method reduces the training time of the used machine learning algorithm.},
  article-number = {1830},
  doi            = {10.3390/app12041830},
  file           = {:/home/rashid/hbrs/rnd_pipe/papers/Best Frame Selection to Enhance Training Step Efficiency in Video-Based Human Action Recognition:PDF},
  url            = {https://www.mdpi.com/2076-3417/12/4/1830},
}

@InProceedings{Ren2020,
  author    = {Ren, Jian and Shen, Xiaohui and Lin, Zhe and Měch, Radomír},
  booktitle = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Best Frame Selection in a Short Video},
  year      = {2020},
  pages     = {3201-3210},
  doi       = {10.1109/WACV45572.2020.9093615},
  file      = {:/home/rashid/hbrs/rnd_pipe/papers/Best_Frame_Selection_in_a_Short_Video_WACV_2020_paper.pdf:PDF},
}

@Article{Ragavan2020,
  author       = {Ragavan, K and Venkatalakshmi, K and Vijayalakshmi, K},
  journal      = {Perspectives in Communication, Embedded-systems and Signal-processing - PiCES},
  title        = {A Case Study of Key Frame Extraction in Video Processing},
  year         = {2020},
  month        = {Aug.},
  number       = {4},
  pages        = {17-20},
  volume       = {4},
  abstractnote = {&lt;p&gt;Video is an integral part of our everyday lives and in too many fields such as content-based video browsing, compression, video analysing, etc., Video has a complex structure that includes scene, shot, and frame. One of the fundamental techniques in content-based video browsing is key frame extraction. In general, to minimize redundancy the key frame should be representative of the video content. A video can be more than one keyframes. The utilization of key frame extraction method speeds up the framework by choosing fundamental frames and thereby removing additional computation on redundant frames. Key frame extraction significantly reduces the video processing overhead time and increase the throughput. In this paper different types of key frame extraction methods are compared with their advantages and disadvantages.&lt;/p&gt;},
  doi          = {10.5281/zenodo.3974504},
  file         = {:/home/rashid/hbrs/rnd_pipe/papers/A Case Study of Key Frame Extraction in Video Provcessing.pdf:PDF},
  url          = {http://www.pices-journal.com/ojs/index.php/pices/article/view/268},
}

@Article{Tang2023,
  author     = {Tang, Hao and Ding, Lei and Wu, Songsong and Ren, Bin and Sebe, Nicu and Rota, Paolo},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {Deep Unsupervised Key Frame Extraction for Efficient Video Classification},
  year       = {2023},
  issn       = {1551-6857},
  month      = {feb},
  number     = {3},
  volume     = {19},
  abstract   = {Video processing and analysis have become an urgent task, as a huge amount of videos (e.g., YouTube, Hulu) are uploaded online every day. The extraction of representative key frames from videos is important in video processing and analysis since it greatly reduces computing resources and time. Although great progress has been made recently, large-scale video classification remains an open problem, as the existing methods have not well balanced the performance and efficiency simultaneously. To tackle this problem, this work presents an unsupervised method to retrieve the key frames, which combines the convolutional neural network and temporal segment density peaks clustering. The proposed temporal segment density peaks clustering is a generic and powerful framework, and it has two advantages compared with previous works. One is that it can calculate the number of key frames automatically. The other is that it can preserve the temporal information of the video. Thus, it improves the efficiency of video classification. Furthermore, a long short-term memory network is added on the top of the convolutional neural network to further elevate the performance of classification. Moreover, a weight fusion strategy of different input networks is presented to boost performance. By optimizing both video classification and key frame extraction simultaneously, we achieve better classification performance and higher efficiency. We evaluate our method on two popular datasets (i.e., HMDB51 and UCF101), and the experimental results consistently demonstrate that our strategy achieves competitive performance and efficiency compared with the state-of-the-art approaches.},
  address    = {New York, NY, USA},
  articleno  = {119},
  doi        = {10.1145/3571735},
  file       = {:/home/rashid/hbrs/rnd_pipe/papers/Deep Unsupervised Key Frame Extraction for Efficient Video Classification.pdf:PDF},
  issue_date = {May 2023},
  keywords   = {unsupervised learning, video classification, Key frame extraction, LSTM, density peaks clustering, weight fusion},
  numpages   = {17},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3571735},
}

@Article{Ouyang2018,
  author    = {S Z Ouyang and L Zhong and R Q Luo},
  journal   = {IOP Conference Series: Materials Science and Engineering},
  title     = {The comparison and analysis of extracting video key frame},
  year      = {2018},
  month     = {may},
  number    = {1},
  pages     = {012010},
  volume    = {359},
  abstract  = {Video key frame extraction is an important part of the large data processing. Based on the previous work in key frame extraction, we summarized four important key frame extraction algorithms, and these methods are largely developed by comparing the differences between each of two frames. If the difference exceeds a threshold value, take the corresponding frame as two different keyframes. After the research, the key frame extraction based on the amount of mutual trust is proposed, the introduction of information entropy, by selecting the appropriate threshold values into the initial class, and finally take a similar mean mutual information as a candidate key frame. On this paper, several algorithms is used to extract the key frame of tunnel traffic videos. Then, with the analysis to the experimental results and comparisons between the pros and cons of these algorithms, the basis of practical applications is well provided.},
  doi       = {10.1088/1757-899X/359/1/012010},
  file      = {:/home/rashid/hbrs/rnd_pipe/papers/The comparison and analysis of extracting video key frame.pdf:PDF},
  publisher = {IOP Publishing},
  url       = {https://dx.doi.org/10.1088/1757-899X/359/1/012010},
}

@Article{Cheng2017,
  author    = {Cheng, Hui and Shi, Yunyu and Yang, Haisheng and Gong, Ming and Liu, Xiang and Xia, Yongxiang},
  journal   = {Journal of Electrical and Computer Engineering},
  title     = {A Fast and Robust Key Frame Extraction Method for Video Copyright Protection},
  year      = {2017},
  issn      = {2090-0147},
  pages     = {1231794},
  volume    = {2017},
  abstract  = {The paper proposes a key frame extraction method for video copyright protection. The fast and robust method is based on frame difference with low level features, including color feature and structure feature. A two-stage method is used to extract accurate key frames to cover the content for the whole video sequence. Firstly, an alternative sequence is got based on color characteristic difference between adjacent frames from original sequence. Secondly, by analyzing structural characteristic difference between adjacent frames from the alternative sequence, the final key frame sequence is obtained. And then, an optimization step is added based on the number of final key frames in order to ensure the effectiveness of key frame extraction. Compared with the previous methods, the proposed method has advantage in computation complexity and robustness on several video formats, video resolution, and so on.},
  doi       = {10.1155/2017/1231794},
  file      = {:/home/rashid/hbrs/rnd_pipe/papers/A Fast and Robust Key Frame Extraction Method for Video Copyright Protection.pdf:PDF},
  publisher = {Hindawi},
  url       = {https://doi.org/10.1155/2017/1231794},
}

@InProceedings{Wu2019,
  author    = {Z. Wu and C. Xiong and C. Ma and R. Socher and L. S. Davis},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {AdaFrame: Adaptive Frame Selection for Fast Video Recognition},
  year      = {2019},
  address   = {Los Alamitos, CA, USA},
  month     = {jun},
  pages     = {1278-1287},
  publisher = {IEEE Computer Society},
  abstract  = {We present AdaFrame, a framework that adaptively selects relevant frames on a per-input basis for fast video recognition. AdaFrame contains a Long Short-Term Memory network augmented with a global memory that provides context information for searching which frames to use over time. Trained with policy gradient methods, AdaFrame generates a prediction, determines which frame to observe next, and computes the utility, i.e., expected future rewards, of seeing more frames at each time step. At testing time, AdaFrame exploits predicted utilities to achieve adaptive lookahead inference such that the overall computational costs are reduced without incurring a decrease in accuracy. Extensive experiments are conducted on two large-scale video benchmarks, FCVID and ActivityNet. AdaFrame matches the performance of using all frames with only 8.21 and 8.65 frames on FCVID and ActivityNet, respectively. We further qualitatively demonstrate learned frame usage can indicate the difficulty of making classification decisions; easier samples need fewer frames while harder ones require more, both at instance-level within the same class and at class-level among different categories.},
  doi       = {10.1109/CVPR.2019.00137},
  file      = {:/home/rashid/hbrs/rnd_pipe/papers/AdaFrame_Adaptive_Frame_Selection_for_Fast_Video_Recognition_CVPR_2019_paper.pdf:PDF},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2019.00137},
}

@InProceedings{Dirfaux2000,
  author    = {Dirfaux, F.},
  booktitle = {Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)},
  title     = {Key frame selection to represent a video},
  year      = {2000},
  pages     = {275-278 vol.2},
  volume    = {2},
  doi       = {10.1109/ICIP.2000.899354},
  file      = {:/home/rashid/hbrs/rnd_pipe/papers/Key_frame_selection_to_represent_a_video.pdf:PDF},
}

@Article{Wu2022,
  author  = {Wu, Zuxuan and Li, Hengduo and Xiong, Caiming and Jiang, Yu-Gang and Davis, Larry S.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {A Dynamic Frame Selection Framework for Fast Video Recognition},
  year    = {2022},
  number  = {4},
  pages   = {1699-1711},
  volume  = {44},
  doi     = {10.1109/TPAMI.2020.3029425},
  file    = {:/home/rashid/hbrs/rnd_pipe/papers/A_Dynamic_Frame_Selection_Framework_for_Fast_Video_Recognition.pdf:PDF},
}

@Article{Sadiq2020,
  author  = {Sadiq, Bashir and Muhammad, Bilyamin and Abdullahi, Muhammad and Onuh, Gabriel and Ali, Abdulhakeem and Babatunde, Adeogun},
  journal = {ELEKTRIKA- Journal of Electrical Engineering},
  title   = {Keyframe Extraction Techniques: A Review},
  year    = {2020},
  month   = {01},
  pages   = {54-60},
  volume  = {19},
  file    = {:/home/rashid/hbrs/rnd_pipe/papers/Keyframe Extraction Techniques\: A Review.pdf:PDF},
  url     = {https://www.researchgate.net/publication/348169269_Keyframe_Extraction_Techniques_A_Review},
}

@InProceedings{8241675,
  author    = {Zhang, Mi and Tian, Lihua and Li, Chen},
  booktitle = {2017 IEEE International Symposium on Multimedia (ISM)},
  title     = {Key Frame Extraction Based on Entropy Difference and Perceptual Hash},
  year      = {2017},
  month     = {Dec},
  pages     = {557-560},
  abstract  = {Key frame extraction is a crucial step in content-based video retrieval. To accurately describe character of frames, various features like color, texture, shape can be integrated and used for key frame extraction. In this paper, we proposed an improved two-phase approach of key frame extraction based on entropy and perceptual hash. It weakens the threshold's direct influence on final results, and solves the problem of fading, sunlight and other information easily resulting in redundant key frames. Firstly, candidate key frames are selected with the use of golden-section partition and weighted histogram. Next, key frames are determined by the entropy values of candidate frames. Finally, a new method of perceptual hash is applied to remove redundant key frames. Experimental data set is created with videos from different domains like movie, cartoon, news etc. Results show that the proposed method is accurate and effective for key frame extraction. The selected key frames can be a good representative of main content.},
  doi       = {10.1109/ISM.2017.109},
  file      = {:/home/rashid/hbrs/rnd_pipe/papers/Key frame extraction based on entropy difference and perceptual hash.pdf:PDF},
}

@InProceedings{Rathod2013,
  author = {I. Rathod and Ashish S. Nikam and Junaid Magdum},
  title  = {An Algorithm for Shot Boundary Detection and Key Frame Extraction Using Histogram Difference},
  year   = {2013},
  file   = {:/home/rashid/hbrs/rnd_pipe/papers/An Algorithm for Shot Boundary Detection and Key Frame Extraction Using Histogram Difference.pdf:PDF},
  url    = {https://api.semanticscholar.org/CorpusID:2830455},
}

@InProceedings{6980938,
  author    = {Liu, Huayong and Hao, Huifen},
  booktitle = {2014 11th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)},
  title     = {Key frame extraction based on improved hierarchical clustering algorithm},
  year      = {2014},
  month     = {Aug},
  pages     = {793-797},
  abstract  = {Key frame greatly reduces the amount of data required in video indexing and provides a suitable abstract for video browsing and retrieval. Key frame extraction plays an important role in content-based video stream analysis, retrieval and inquiry. In order to extract key frame efficiently from different type of videos, in this paper we propose an improved hierarchical clustering algorithm that combing K-means algorithm. The improved hierarchical clustering algorithm is used to obtain an initial clustering result. And K-means is conducted to optimize the initial clustering result and obtain the final clustering result. Finally, the center frame of each clustering is extracted as key frame. Experimental results show that compared with other existing methods, the representations of key frame extracted by our algorithm are better in expressing the primary content of video.},
  doi       = {10.1109/FSKD.2014.6980938},
  file      = {:/home/rashid/hbrs/rnd_pipe/papers/Key frame extraction based on improved hierarchical clustering algorithm.pdf:PDF},
}

@InProceedings{Liu2022,
  author    = {Y. Liu and X. Zhang and Y. Li and G. Liang and Y. Jiang and L. Qiu and H. Tang and F. Xie and W. Yao and Y. Dai and Y. Qiao and Y. Wang},
  booktitle = {2022 26th International Conference on Pattern Recognition (ICPR)},
  title     = {VideoPipe 2022 Challenge: Real-World Video Understanding for Urban Pipe Inspection},
  year      = {2022},
  address   = {Los Alamitos, CA, USA},
  month     = {aug},
  pages     = {4967-4973},
  publisher = {IEEE Computer Society},
  abstract  = {Video understanding is an important problem in computer vision. Currently, the well-studied task in this research is human action recognition, where the clips are manually trimmed from the long videos, and a single class of human action is assumed for each clip. However, we may face more complicated scenarios in the industrial applications. For example, in the real-world urban pipe system, anomaly defects are fine-grained, multi-labeled, domain-relevant. To recognize them correctly, we need to understand the detailed video content. For this reason, we propose to advance research areas of video understanding, with a shift from traditional action recognition to industrial anomaly analysis. In particular, we introduce two high-quality video benchmarks, namely QV-Pipe and CCTV-Pipe, for anomaly inspection in the real-world urban pipe systems. Based on these new datasets, we will host two competitions including (1) Video Defect Classification on QV-Pipe and (2) Temporal Defect Localization on CCTV-Pipe. In this report, we describe the details of these benchmarks, the problem definitions of competition tracks, the evaluation metric, and the result summary. We expect that, this competition would bring new opportunities and challenges for video understanding in smart city and beyond. The details of our VideoPipe challenge can be found in https://videopipe.github.io.},
  doi       = {10.1109/ICPR56361.2022.9956055},
  keywords  = {location awareness;measurement;computer vision;machine learning algorithms;smart cities;face recognition;benchmark testing},
  url       = {https://doi.ieeecomputersociety.org/10.1109/ICPR56361.2022.9956055},
}

@InProceedings{723655,
  author    = {Yueting Zhuang and Yong Rui and Huang, T.S. and Mehrotra, S.},
  booktitle = {Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)},
  title     = {Adaptive key frame extraction using unsupervised clustering},
  year      = {1998},
  month     = {Oct},
  pages     = {866-870 vol.1},
  volume    = {1},
  abstract  = {Key frame extraction has been recognized as one of the important research issues in video information retrieval. Although progress has been made in key frame extraction, the existing approaches are either computationally expensive or ineffective in capturing salient visual content. We first discuss the importance of key frame selection; and then review and evaluate the existing approaches. To overcome the shortcomings of the existing approaches, we introduce a new algorithm for key frame extraction based on unsupervised clustering. The proposed algorithm is both computationally simple and able to adapt to the visual content. The efficiency and effectiveness are validated by large amount of real-world videos.},
  doi       = {10.1109/ICIP.1998.723655},
}

@Article{Gianluigi2006AnIA,
  author  = {Ciocca Gianluigi and Schettini Raimondo},
  journal = {Journal of Real-Time Image Processing},
  title   = {An innovative algorithm for key frame extraction in video summarization},
  year    = {2006},
  pages   = {69-88},
  volume  = {1},
  url     = {https://api.semanticscholar.org/CorpusID:62760522},
}

@Article{Sheena2015,
  author   = {Sheena {C V} and N.K. Narayanan},
  journal  = {Procedia Computer Science},
  title    = {Key-frame Extraction by Analysis of Histograms of Video Frames Using Statistical Methods},
  year     = {2015},
  issn     = {1877-0509},
  note     = {Proceedings of the 4th International Conference on Eco-friendly Computing and Communication Systems},
  pages    = {36-40},
  volume   = {70},
  abstract = {Summarization of videos for different applications like video object recognition and classification, video retrieval and archival and surveillance is an active research area in computer vision. One of the methods to summarize video data is extraction of key-frame. This paper proposes a method of key-frame extraction using thresholding of absolute difference of histogram of consecutive frames of video data. The experiment is conducted on KTH action database. For evaluation purpose compression ratio and fidelity value is calculated and it is able to achieve reasonably higher accuracy rate.},
  doi      = {https://doi.org/10.1016/j.procs.2015.10.021},
  keywords = {Key-frame, video summarization, absolute difference, histogram, thresholding},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050915031853},
}

@Comment{jabref-meta: databaseType:bibtex;}
